{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d8a026-5757-4667-8ebc-f4e4713adcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install scipy==1.12\n",
    "!pip install gensim\n",
    "#!pip install allennlp\n",
    "!pip install transformers\n",
    "!pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae41b942-9ea8-4947-bf66-4bd82ae28e00",
   "metadata": {},
   "source": [
    "# Веторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5a3f77-f115-4406-9d0d-285f7cada04c",
   "metadata": {},
   "source": [
    "Векторизация текста — это процесс преобразования текстовых данных в числовые представления, которые могут быть использованы алгоритмами машинного обучения и обработки естественного языка. Существуют различные методы векторизации текста, начиная с простых и заканчивая более сложными и мощными моделями. Вот обзор основных методов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c59209-2769-42b5-9750-692feacba16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Кошка сидит на подоконнике и смотрит на улицу.\",\n",
    "    \"Дети играют в парке с мячом.\",\n",
    "    \"На столе лежат книги и тетради.\",\n",
    "    \"Птицы поют в саду утром.\",\n",
    "    \"Машина быстро едет по шоссе.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd946e3-be06-42b4-87ce-0734d696c088",
   "metadata": {},
   "source": [
    "## Bag of word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0762acb8-7844-458e-9ec2-c937c7f5cd8f",
   "metadata": {},
   "source": [
    "**Описание:** Преобразование текста в вектор фиксированной длины, где каждая позиция соответствует отдельному слову из всего корпуса, а значение — количество вхождений слова в документ.\n",
    "\n",
    "**Плюсы:**\n",
    "- Простота реализации.\n",
    "- Интуитивная интерпретация.\n",
    "\n",
    "**Минусы:**\n",
    "- Не учитывает порядок слов.\n",
    "- Сложность возрастает с увеличением размера корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d4a4009-8ad0-4d5c-ac7b-766e5c1c55a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words:\n",
      "['быстро' 'дети' 'едет' 'играют' 'книги' 'кошка' 'лежат' 'машина' 'мячом'\n",
      " 'на' 'парке' 'по' 'подоконнике' 'поют' 'птицы' 'саду' 'сидит' 'смотрит'\n",
      " 'столе' 'тетради' 'улицу' 'утром' 'шоссе']\n",
      "[[0 0 0 0 0 1 0 0 0 2 0 0 1 0 0 0 1 1 0 0 1 0 0]\n",
      " [0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0]\n",
      " [1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Создание BoW модели\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Показать результат\n",
    "print(\"Bag of Words:\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04536c4-7c8c-4e91-afb3-c3b7b692718c",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee203ad2-2706-4c7f-b985-3247e8db65a9",
   "metadata": {},
   "source": [
    "**Описание:** Улучшение BoW, где каждый элемент вектора весится по частоте слова в документе и обратной частоте слова в корпусе.\n",
    "\n",
    "**Плюсы:**\n",
    "- Уменьшает влияние часто встречающихся слов (например, «и», «в»).\n",
    "- Лучше различает значимые слова.\n",
    "\n",
    "**Минусы:**\n",
    "- Не учитывает порядок слов.\n",
    "- Меньше информации о контексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18d76319-1f2a-4e8c-bfff-b1ef04166c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF:\n",
      "['быстро' 'дети' 'едет' 'играют' 'книги' 'кошка' 'лежат' 'машина' 'мячом'\n",
      " 'на' 'парке' 'по' 'подоконнике' 'поют' 'птицы' 'саду' 'сидит' 'смотрит'\n",
      " 'столе' 'тетради' 'улицу' 'утром' 'шоссе']\n",
      "[[0.         0.         0.         0.         0.         0.36265071\n",
      "  0.         0.         0.         0.58516862 0.         0.\n",
      "  0.36265071 0.         0.         0.         0.36265071 0.36265071\n",
      "  0.         0.         0.36265071 0.         0.        ]\n",
      " [0.         0.5        0.         0.5        0.         0.\n",
      "  0.         0.         0.5        0.         0.5        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.46369322 0.\n",
      "  0.46369322 0.         0.         0.37410477 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.46369322 0.46369322 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5        0.5        0.5        0.         0.\n",
      "  0.         0.         0.         0.5        0.        ]\n",
      " [0.4472136  0.         0.4472136  0.         0.         0.\n",
      "  0.         0.4472136  0.         0.         0.         0.4472136\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.4472136 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Создание TF-IDF модели\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Показать результат\n",
    "print(\"TF-IDF:\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8280038-5737-4e47-9b0b-f5de0b4d610e",
   "metadata": {},
   "source": [
    "# Word Embeddings (Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa342ca9-605d-4672-b65c-212a4648944b",
   "metadata": {},
   "source": [
    "**Описание:** Преобразование слов в плотные векторы фиксированной длины, обученные на большом корпусе текстов. Вектора схожих по смыслу слов расположены близко друг к другу в векторном пространстве.\n",
    "\n",
    "**Плюсы:**\n",
    "- Учитывает семантическую информацию.\n",
    "- Компактное представление.\n",
    "\n",
    "**Минусы:**\n",
    "- Не учитывает полисемию (многозначность слов).\n",
    "- Статическое представление (каждое слово всегда представлено одним и тем же вектором).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df5f2515-34ee-4c42-a7cc-9a8ecf946ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec:\n",
      "[-6.9636069e-03 -2.4585116e-03 -8.0229370e-03  7.5005279e-03\n",
      "  6.1274157e-03  5.2584694e-03  8.3778575e-03 -6.9653272e-04\n",
      " -9.3127284e-03  9.1156662e-03 -4.9285362e-03  7.8479899e-03\n",
      "  5.5338596e-03 -1.0790766e-03 -7.6642158e-03 -1.4598024e-03\n",
      "  6.2535368e-03 -6.9660828e-03  1.4420962e-03 -7.9518585e-03\n",
      "  8.7213479e-03 -2.8557885e-03  9.4373021e-03 -5.7080747e-03\n",
      " -9.7177243e-03 -8.6279036e-03 -4.0748348e-03  4.7095944e-03\n",
      " -2.4193883e-04  9.2235124e-03  3.1092144e-03  3.7477673e-03\n",
      "  2.9963492e-03  8.1486488e-03 -2.3967146e-03  7.4073388e-03\n",
      " -9.5367134e-03  2.9210865e-03 -6.8166968e-04  4.5225740e-04\n",
      "  6.8430100e-03 -2.8419732e-03 -2.3567795e-03 -1.0047674e-04\n",
      " -4.9769162e-04 -3.5749613e-03  6.2444829e-03 -6.5586674e-03\n",
      "  7.8919996e-03 -9.3460083e-05  2.6088404e-03  3.2231498e-03\n",
      " -2.8165340e-04  1.7063022e-03 -3.1406546e-03  4.7564553e-03\n",
      "  2.4301052e-04 -3.2805956e-03 -8.7145744e-03 -9.9980794e-03\n",
      "  3.1277776e-04 -5.7468102e-03 -1.1096597e-03 -4.2060935e-03\n",
      " -8.6388253e-03  1.0620963e-03  5.9110904e-03 -2.2109700e-03\n",
      " -7.1708169e-03  3.1534373e-03 -3.8468599e-04 -5.5211424e-03\n",
      " -1.1056293e-03 -6.3965440e-04 -3.1830894e-03 -9.9550774e-03\n",
      "  7.6385941e-03  3.7260079e-03 -2.5292134e-03  7.3071741e-03\n",
      "  4.5459031e-04  7.1731522e-03 -1.5475631e-03  7.4936678e-03\n",
      " -4.2631626e-05 -6.0773613e-03 -4.7158506e-03  9.6284244e-03\n",
      "  5.8106182e-04  1.0274005e-03  8.4502874e-03 -6.2884046e-03\n",
      " -1.7638588e-03 -8.1789196e-03 -6.6747544e-03 -8.5804872e-03\n",
      "  3.9306106e-03  2.7393461e-03  5.6154132e-03  2.5717581e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Разбиение текстов на слова\n",
    "tokenized_texts = [text.split() for text in texts]\n",
    "\n",
    "# Создание Word2Vec модели\n",
    "model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Получение вектора для слова \"Кошка\"\n",
    "vector = model.wv['Кошка']\n",
    "print(\"Word2Vec:\")\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf6cd7b-e0c1-4d80-bdfe-3cb2c3ddddad",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c62ed5-e614-44d1-87a8-9f6d4580e6b2",
   "metadata": {},
   "source": [
    "**Описание:** Расширение Word2Vec, учитывающее морфологию слов за счет использования n-грамм символов. Это позволяет лучше работать с редкими и нечасто встречающимися словами.\n",
    "\n",
    "**Плюсы:**\n",
    "- Учитывает внутреннюю структуру слов.\n",
    "- Обрабатывает неизвестные слова.\n",
    "\n",
    "**Минусы:**\n",
    "- Увеличивает сложность и размер модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60f4ad4b-e864-4f6e-8805-6fa396d488dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText vector for 'Кошка':\n",
      "[-3.0525922e-04  3.5336034e-03  7.2508253e-04 -1.2815356e-03\n",
      " -1.6631660e-03 -7.8654301e-04 -4.4977730e-03  1.0739085e-03\n",
      "  7.8009005e-05 -4.1312369e-04 -8.3559775e-04 -6.6402521e-05\n",
      "  1.3927444e-03 -5.2920578e-04 -9.9934143e-05 -3.5590067e-04\n",
      "  3.3396761e-06 -2.2929015e-03 -1.4277304e-03  1.0141414e-04\n",
      " -1.9697933e-03  5.4989534e-04  5.4035580e-04 -2.6456742e-03\n",
      "  6.4059469e-04  2.1918833e-03 -2.7556324e-03  4.7027759e-04\n",
      " -8.4990542e-04 -5.8188400e-04  3.7306064e-04  1.6517009e-04\n",
      "  1.6612363e-03 -2.1993710e-05 -1.1146673e-03  3.1653314e-04\n",
      "  2.3638161e-03 -3.8904304e-04  2.3746665e-03 -1.2454481e-03\n",
      "  6.7963731e-05  4.1478415e-04  2.2588321e-03  4.3581426e-03\n",
      "  1.1857841e-03  4.6460010e-04 -1.3296860e-03 -9.5804346e-05\n",
      "  2.7320541e-03  2.1892695e-03 -1.7284085e-03  2.0815746e-03\n",
      " -2.5513375e-03  1.7236081e-03 -2.1386971e-03 -5.0960545e-04\n",
      " -2.0002373e-03 -1.0356142e-03  1.8930421e-04 -2.0051128e-03\n",
      "  1.1864470e-03 -8.6721608e-05 -2.1993715e-04  8.5411593e-04\n",
      " -2.3729170e-03 -4.7089526e-04  4.4571052e-04 -1.8662229e-05\n",
      "  1.4704869e-03 -4.4890132e-04 -3.8033022e-04  6.9247111e-04\n",
      " -1.1913436e-03 -5.3021248e-04 -3.9999501e-04  4.4677546e-04\n",
      "  1.2045401e-03  1.6068399e-03 -1.3996313e-03 -1.0148128e-03\n",
      " -9.8898541e-04 -3.8796026e-04  6.8903057e-04  1.6970051e-03\n",
      "  6.5949239e-04 -1.4145920e-03 -2.5971403e-04 -6.8665779e-04\n",
      " -4.1821346e-04  1.6414631e-03 -3.3457857e-04  1.9299375e-03\n",
      "  3.1763394e-04  2.2382822e-03  5.2715535e-04 -2.4228410e-03\n",
      " -2.2816556e-03 -4.1848698e-04 -2.6105493e-04 -1.2882970e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Пример текстов\n",
    "texts = [[\"Кошка\", \"сидит\", \"на\", \"подоконнике\"], [\"Дети\", \"играют\", \"в\", \"парке\"], [\"Машина\", \"едет\", \"по\", \"шоссе\"]]\n",
    "\n",
    "# Создание FastText модели\n",
    "model = FastText(sentences=texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Получение вектора для слова \"Кошка\"\n",
    "vector = model.wv['Кошка']\n",
    "print(\"FastText vector for 'Кошка':\")\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c1c613-e016-44d3-a816-58c9ecb4b491",
   "metadata": {},
   "source": [
    "# Transformers (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5a56d-74ce-455f-907d-718df1f73415",
   "metadata": {},
   "source": [
    "**Описание:** Используют механизмы внимания (attention) для создания контекстуально зависимых представлений слов. Эти модели могут быть предварительно обучены на большом корпусе текстов и затем дообучены на конкретных задачах.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6232a83d-0ce7-430a-9263-79f4d15100ad",
   "metadata": {},
   "source": [
    "**BERT (Bidirectional Encoder Representations from Transformers):**\n",
    "- **Описание:** Использует двунаправленное обучение, учитывая контекст слова как слева, так и справа.\n",
    "- **Плюсы:** Отлично справляется с задачами понимания текста.\n",
    "- **Минусы:** Ресурсоемкий процесс обучения и использования.\n",
    "\n",
    "**GPT (Generative Pre-trained Transformer):**\n",
    "- **Описание:** Использует однонаправленное обучение, подходит для генерации текста.\n",
    "- **Плюсы:** Хорошо генерирует связный текст.\n",
    "- **Минусы:** Меньше контекстной информации по сравнению с двунаправленными моделями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9cf068-cbce-436e-a452-d5c059faa878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Пример одного предложения\n",
    "sentence = texts[0]\n",
    "\n",
    "# Создание токенайзера и модели\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Токенизация и получение тензоров\n",
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Получение векторов для каждого токена\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(\"BERT:\")\n",
    "print(last_hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2e74d9-28e3-4d46-8770-75c4152efb29",
   "metadata": {},
   "source": [
    "# Sentence Embeddings (Sentence-BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ea2500-d74a-400f-a25a-8989cb712c95",
   "metadata": {},
   "source": [
    "\n",
    "**Описание:** Преобразование предложений или текстов в векторы фиксированной длины, учитывая их полное содержание.\n",
    "\n",
    "**Плюсы:**\n",
    "- Учитывает семантику всего предложения.\n",
    "- Подходит для задач сравнения текстов и поиска.\n",
    "\n",
    "**Минусы:**\n",
    "- Требует больших объемов данных для обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c30648c-1a06-4513-9b88-528b3084164d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.39828658 -0.23388262 -0.00871656 ... -0.06895408 -0.02948338\n",
      "   0.19009201]\n",
      " [ 0.22489727  0.0578384   0.03988566 ...  0.1589544   0.03482928\n",
      "   0.3165689 ]\n",
      " [ 0.29827043  0.11546078 -0.04700516 ... -0.01434096 -0.02188937\n",
      "   0.12155327]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Создание модели\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Получение векторов\n",
    "embeddings = model.encode(texts)\n",
    "\n",
    "# Показать результат\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd5c81b-5a01-40b4-aff3-ead1b3c5235a",
   "metadata": {},
   "source": [
    "# Pre-trained Language Models (GPT-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7374912a-458d-4c2c-8c7b-13e194f30fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Установите ваш API ключ\n",
    "openai.api_key = 'your-api-key'\n",
    "\n",
    "# Пример текста\n",
    "text = texts[0]\n",
    "\n",
    "# Запрос к GPT-3 для получения векторов\n",
    "response = openai.Embedding.create(input=text, engine=\"text-embedding-ada-002\")\n",
    "embedding = response['data'][0]['embedding']\n",
    "\n",
    "# Показать результат\n",
    "print(embedding)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
