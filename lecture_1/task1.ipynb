{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde72582-41e0-4c8c-95a2-5ddbd267d488",
   "metadata": {},
   "source": [
    "# Лабараторная работа 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e17685f-a5b9-4af6-891f-c9784698ecaa",
   "metadata": {},
   "source": [
    "## Часть 1 - сбор данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4738cd-eb29-4945-828b-58603baf5f9c",
   "metadata": {},
   "source": [
    "Суть - написать парсер текстовых данных (статей)\n",
    "Нужно использовать 1 из предложенных вариантов (а также можно использовать свой пример)\n",
    "\n",
    "Варианты сайтов для парсинга:\n",
    "1. https://lenta.ru\n",
    "2. https://naked-science.ru/\n",
    "3. https://ngs24.ru/\n",
    "4. https://goldenmost.ru/\n",
    "5. https://nta-pfo.ru/\n",
    "\n",
    "Можно использовать любой другой источник.\n",
    "\n",
    "Необходимо сохранить следующие сущности\n",
    "1. title - заголовок статьи\n",
    "2. content - текст статьи\n",
    "3. category - категория статьи\n",
    "4. comments - (при наличии)\n",
    "5. created_date - дата создания статьи (при наличии)\n",
    "6. // прочие данные, которые вы посчитаете полезной)\n",
    "\n",
    "Примеры парсеров расположены в lecture_1:\n",
    "1. 1_sync_parser - самый простой и топорный вариант\n",
    "2. 2_thread_parser - парсер основанный на многопоточности\n",
    "3. 3_async_parser - парсер основанный на асинхронности\n",
    "4. 4_browser_parser - парсер основанный на создании вирртуального парсера (полезно, когда сайт основан на client-side rendering)\n",
    "\n",
    "P.S. Для запуска - необходимо установить python 3.11 (https://www.python.org/downloads/release/python-3110/)\n",
    "\n",
    "Установить зависимости\n",
    "```pip3 install -r requirements.txt```\n",
    "\n",
    "Запуск файлов\n",
    "``` python3 lecture_1/1_sync_parser/main.py ```\n",
    "\n",
    "\n",
    "Для тех кто под *, необходимо использовать варианты 3,4 либо использовать фреймворки для парсинга, например **scrappy** (https://scrapy.org/)\n",
    "P.S. запрещено использовать naked-science.ru если вы используете scrappy (пример кода [парсинг naked-science.ru](https://disk.yandex.ru/d/rsZ2sUvjj4lfUg))\n",
    "\n",
    "Разрешенные форматы хранения результатов\n",
    "- .csv\n",
    "- .json\n",
    "- .sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707fd135-0fc5-4c57-9f35-85641dae7575",
   "metadata": {},
   "source": [
    "## Часть 2 - анализ текстовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482571a2-f872-4b71-a62b-56e5032e094c",
   "metadata": {},
   "source": [
    "Провести аналитику текстовых данных.\n",
    "\n",
    "1. Токенезировать и лемматизировать полученные текстовые данные полученные из 1 части задания\n",
    "\n",
    "- Токенизация - разделение слов на предложения\n",
    "- Лемматизация - перевод слов в начальную форму\n",
    "\n",
    "2. Посчитать наиболее встречающиеся пары подлежащих и сказуемых\n",
    "3. Посчитать самые популлярные слова (исключая стоп-слова, словарь стоп слов можно найти в nltk, либо отбросить по частям речи)\n",
    "4. Вывести статистику (по убыванию, наиболее встречающиеся пары и самые популрняые стоп слоова)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "837f2000-52cf-4cea-85f1-7e9c5e2b0686",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'text': 'Эти типы стали есть в цеху. Пример токенизации. Для примера мы токенезируем'}, \n",
    "    {'text': 'Научный советник рассказал о... (читать продолжение в источнике).'}, \n",
    "    {'text': 'В статье говориться о котиках.'},\n",
    "    {'text': 'Стекло бьется.'}\n",
    "] # p.s. здесь вы открываете полученные данные\n",
    "\n",
    "# Для примера можно использовать natasha\n",
    "# !pip install natasha\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "names_extractor = NamesExtractor(morph_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04f58935-d3fc-4be0-b304-c183e32e2a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ┌► Эти   det\n",
      "    ┌►└─ типы  nsubj\n",
      "┌─┌─└─┌─ стали \n",
      "│ │   └► есть  xcomp\n",
      "│ │   ┌► в     case\n",
      "│ └──►└─ цеху  obl\n",
      "└──────► .     punct\n",
      "   Пример      \n",
      "┌─ токенизации \n",
      "└► .           punct\n",
      "  ┌► Для          case\n",
      "┌►└─ примера      obl\n",
      "│ ┌► мы           nsubj\n",
      "└─└─ токенезируем \n",
      "    ┌► Научный   amod\n",
      "  ┌►└─ советник  nsubj\n",
      "┌─└─┌─ рассказал \n",
      "│   └► о         advmod\n",
      "└────► ...       punct\n",
      "        ┌► (           punct\n",
      "┌─┌───┌─└─ читать      \n",
      "│ │ ┌─└──► продолжение obj\n",
      "│ │ │   ┌► в           case\n",
      "│ │ └──►└─ источнике   nmod\n",
      "│ └──────► )           punct\n",
      "└────────► .           punct\n",
      "      ┌► В          case\n",
      "    ┌►└─ статье     obl\n",
      "┌─┌─└─── говориться \n",
      "│ │   ┌► о          case\n",
      "│ └──►└─ котиках    obl\n",
      "└──────► .          punct\n",
      "  ┌► Стекло obj\n",
      "┌─└─ бьется \n",
      "└──► .      punct\n"
     ]
    }
   ],
   "source": [
    "from ipymarkup import show_dep_ascii_markup as show_markup\n",
    "for article in data:\n",
    "    text = article.get('text')\n",
    "    \n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.parse_syntax(syntax_parser)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    for sentence in doc.sents: # получаем предложения\n",
    "        sentence.syntax.print()\n",
    "#         for token in sentence.tokens: # получаем токены\n",
    "#             print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2684f184-9807-4fea-92c5-c6ceee30063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Продолжение следует..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275291be-52cc-4f56-becc-8a78ed7bf34d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
